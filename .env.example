# MaxQ Environment Configuration
# Copy this file to .env and fill in your values

# ============================================
# MODE SELECTION
# ============================================

# MaxQ Mode: cloud, local, tei, or auto (default)
#   cloud - Use Qdrant Cloud Inference (embeddings computed server-side)
#   local - Use FastEmbed locally (limited to ~20 pre-converted models)
#   tei   - Use Text Embeddings Inference server (ANY HuggingFace model)
#   auto  - Detect: TEI_URL > cloud URL > local
MAXQ_MODE=auto

# TEI Server URL (for tei mode - run ANY HuggingFace embedding model)
# Start TEI: docker run -p 8080:80 ghcr.io/huggingface/text-embeddings-inference:latest --model-id BAAI/bge-large-en-v1.5
MAXQ_TEI_URL=

# ============================================
# QDRANT CONNECTION
# ============================================

# Qdrant Vector Database
# For Cloud: https://your-cluster.cloud.qdrant.io
# For Docker: http://qdrant:6333 (in docker-compose)
# For Local: http://localhost:6333
QDRANT_URL=http://localhost:6333

# Qdrant API key (required for Cloud, optional for local)
QDRANT_API_KEY=

# ============================================
# OPTIONAL
# ============================================

# OpenAI API Key (for LLM-powered assertions and RAG features)
# Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Linkup SDK (for web search integration)
# Get yours at: https://linkup.so
LINKUP_API_KEY=

# ============================================
# ENGINE (Rust acceleration)
# ============================================

# Engine mode: auto, true, false
MAXQ_USE_ENGINE=auto

# Engine gRPC URL
MAXQ_ENGINE_URL=localhost:50051

# ============================================
# ADVANCED
# ============================================

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO
